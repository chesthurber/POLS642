---
title: "W2L2: Theoretical Distributions"
subtitle: "POLS 642 Intermediate Analysis of Political Data"
author: "Dr. Ches Thurber"
format: 
  revealjs:
    theme: simple
execute: 
  echo: true
editor: visual
---

## Get Started

1\) Follow link on Blackboard to download `W2L2_Theoretical_Distributions.zip`

2\) "Unzip" it and get the files set up in your file directory the way you want them

3\) Open up your R Project

## Plan for Today

1\) Recap

2\) Describing Data Statistically

3\) Describing Data Visually (Distribution of our Sample)

4\) Theoretical Distributions (Distribution of the Population?)

# Recap

## Populations and Samples

-   *Population*: all of some unit of analysis that exists (or could exist)
-   *Sample*: a subset of the population that we are able to measure (our dataset)
-   *Observation*: a single unit from our sample (row in our dataset)
-   *Variable*: measured attributes of each observation (columns in our dataset)

## Descriptive Inference

-   Descriptive Inference: estimate the characteristics of the population based on the sample

    -   if the sample is *representative*, then it should offer an *unbiased* estimate of the characteristics of the population

    -   a sample that is *random* is often a good place to start in trying to achieve a sample that is *representative*

## Loading Some Data

1\) Check your working directory

```{r}
getwd()
```

2\) Load the data (visually, or with code)

```{r}
bes <- read.csv("BES.csv")
```

## Look at the Data

Show the first five rows:

```{r}
head(bes)
```

# Describing Data Statistically

## Central Tendency

-   where is the "middle" of the distribution?

-   median: the "middle" observation when you put them in order

-   mean: $$\bar x = \frac{\sum_{i=1}^{n}x_i}{n}$$

## Dispersion

-   min/max

-   quintiles (value of variable at which 10%, 25%, 75%, 90% etc... of observations are lower)

-   standard deviation: like the average distance from the mean $$\sigma = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}}$$

-   variance: squared standard deviation

## Summary Stats at once

```{r}
summary(bes$age)
```

## Standard Deviation and Variance in `R`

```{r}

sd(bes$age, na.rm=T)
var(bes$age, na.rm=T)
```

## 

# Describing Data Visually

## Boxplots

```{r}
library(ggplot2)
boxplot(bes$age)
```

## Histogram

-   a *histogram* is a graphical representation of a variable's distribution

-   it is made of of "bins" of intervals of the variable on the x-axis (left-right)

-   the y-axis (top-bottom) represents the number of observations of the variable within each "binned" interval

## A Histogram of Respondent Age

```{r}
ggplot(bes, aes(x = age)) +
  geom_histogram(color = "black", fill = "lightgray") + 
  theme_bw()
```

## Density Histogram

-   a histogram where the scale of the x-axis is adjusted so that the total "area" adds up to 1

-   density of the bin = proportion of observations in the bin / width of the bin

-   it's useful because we can compare to density histograms of other samples with different numbers of observations

## A Density Histogram of Respondent Age

```{r}

ggplot(bes, aes(x = age)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "lightgray") + 
  theme_bw()
```

## Density Plot

-   take a density histogram and make the bin width smaller and smaller

-   replace the bars with a curvy line

-   the curvy line is such that the area beneath it sums to 1

-   at any point along the x axis, the area under the line to the left represents the percentage of data less than that value of x

## A Density Plot

```{r}
ggplot(bes, aes(x = age)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "lightgray") +
  geom_density() + 
  theme_bw()

```

## Just the Density

```{r}
ggplot(bes, aes(x = age)) +
  geom_density() + 
  theme_bw()
```

# Theoretical Distributions


## Probability Density Functions



- PDFs are mathematical functions that describe the relative probability of a continuous random variable

- You can use a PDF to calculate the probability of a value falling within a specific range

- these are *theoretical*: a way a variable could be distributed, not based on any observed sample of data

## PDF Examples

<img src="PDFs.png" alt="PDFs" width="400"/>

## The Normal Distribution

<img src="Normal.svg" alt="PDFs" width="600"/>

- a specific PDF that is symmetrical and in which 95 percent of outcomes fall within 2 standard deviations from the mean.


## The Binomial Distribution

- the distribution of bivariate outcomes with probability $\pi$
- approximates the normal distribution as size gets large


```{r, coin flips}
coin_flips <- rbinom(n = 100000, size=6, prob = .5)
```

```{r, coin histogram, out.width = '70%'}
hist(coin_flips)
```


## Using a PDF

- We can use the PDF to calculate the probability of specific events

- e.g. What is the probability of flipping heads six times in a row?

```{r, coin calculations}
sum(coin_flips==6)/100000
dbinom(x=6, size=6, prob=0.5)
```


## From Probability to Inference

- with the mean, sd, and PDF in hand, we can calculate or simulate the probability of any set of possible outcomes

- this is going to be at the heart of what we do all semester long

- what will make it tricky is that we will usually be going backward

- that is, we will be looking at some output, and trying to make an inference about the data generating process


## Core Problem

- we can know the probability of a model/distribution producing a certain outcome
    - e.g. "The probability of flipping a fair coin heads twice in a row is .25"

- we cannot know the probability of a model being true given an outcome
    - e.g. "Heads came up twice in a row: what's the probability that it's fair?"
    
## What to do?

- We can take a theoretical distribution and ask "what is the probability that a sample taken from this distribution would look like our sample?"

- if the probability of drawing a sample that looks like ours from the theoretical distribution is reasonably likely, we accept that the theoretical distribution might be a plausible model of the distrobution of the population

- but if the probability of drawing a sample that looks like ours from the theoretical distribution is highly unlikely, we reject the theoretical distribution as a possible model of the population

## Hypothesis Testing approach

- establish **null hypothesis** of what the theoretical distribution of the population might be

- calculate probability of drawing a sample with the properties of your observed sample from the null hypothesis's theoretical distribution

- reject the null hypothesis when the probability of drawing the observed sample (or one more extreme) is small (often $\alpha = 0.05$, but this is totally arbitrary!)

- usually focused on the *mean* as the statistical description of the sample that we evaluate


## Hypothesis testing the Coin flip

- **Null Hypothesis**: The coin is "fair" (i.e. the population distribution is binomial with $\pi$ = 0.5)

- **Alternative Hypothesis**: Thurber is a liar! (i.e. the population distribution is something else)

- after how many continuous heads should you reject the null that the coin is fair with $\alpha = 0.05$?
```{r, four flips}
dbinom(x=4, size=4, prob=0.5)
```
```{r, five flips}
dbinom(x=5, size=5, prob=0.5)
```


## Wrap up

-   Thursday: Theoretical Distributions and Descriptive Inference
-   Due Friday: Data Camp `Hypothesis Testing in R` unit; Optional `Probability` unit
